{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import colors\n",
    "import cartopy.crs as ccrs  # Import cartopy ccrs\n",
    "import cartopy.feature as cfeature  # Import cartopy common features\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "sys.path.insert(0, \"/home/chalifour/code/master\")\n",
    "from fct_script.func_py import get_proj_extent\n",
    "from matplotlib.patches import Patch\n",
    "import fct_script.rpn_funcs_chris as rpn_chris\n",
    "from tqdm.notebook import trange, tqdm\n",
    "try:\n",
    "    import rpnpy.librmn.all as rmn  # Module to read RPN files\n",
    "    from rotated_lat_lon import RotatedLatLon  # Module to project field on native grid (created by Sasha Huziy)\n",
    "except ImportError as err:\n",
    "    print(f\"RPNPY can only be use on the server. It can't be use on a personal computer.\"\n",
    "          f\"\\nError throw :{err}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T17:11:35.778615Z",
     "start_time": "2023-06-12T17:11:35.734775Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Date formating\n",
    "\n",
    "date = '2022-03-25'\n",
    "datetimeobject = datetime.strptime(date, '%Y-%m-%d')\n",
    "new_format = datetimeobject.strftime('%Y%m')\n",
    "#\n",
    "run = pd.Timestamp(fr'{date} 00')\n",
    "\n",
    "f_hr = 48\n",
    "valid_time = run + pd.Timedelta(str(f_hr) + ' h')\n",
    "timestamps = pd.date_range(run, valid_time, freq='1 H')\n",
    "time = timestamps\n",
    "\n",
    "\n",
    "# start = pd.date_range(start='2021-12-6 00',end='2021-12-7 19',freq='2H')\n",
    "# end = pd.date_range(start='2021-12-6 02',end='2021-12-9 20',freq='2H')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T17:11:35.781712Z",
     "start_time": "2023-06-12T17:11:35.778817Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# path formating\n",
    "\n",
    "data_path = sorted(glob.glob(fr\"/upslope/chalifour/projet_maitrise/\"))[0]\n",
    "\n",
    "graph_savepath = os.path.join(data_path, r'fig/graph_UL/data_visualisation')\n",
    "HQ_preformat_savepath = os.path.join(data_path, r\"data_parcivel/station_hq/station_gmon/Preformated_DB\")\n",
    "HQ_compiled_savepath = os.path.join(data_path, r\"data_format-master/Data.nosync/station_gmon/Full_datasets\")\n",
    "\n",
    "DB_file_2022 = glob.glob(HQ_preformat_savepath + '/*.csv')\n",
    "\n",
    "file_15min = glob.glob(HQ_compiled_savepath + '/*_15min.csv')\n",
    "\n",
    "file_1h = glob.glob(HQ_compiled_savepath + '/dataset_1h.csv')\n",
    "\n",
    "\n",
    "# uqam\n",
    "\n",
    "file_uqam_1h = glob.glob(os.path.join(data_path + 'data_format-master/Data.nosync/site_uqam/Full_dataset/dataset_1h.csv'))\n",
    "\n",
    "# momo\n",
    "file_momo_1h = glob.glob(os.path.join(data_path + 'data_format-master/Data.nosync/site_neige/Full_dataset/dataset_1h.csv'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T17:11:35.781809Z",
     "start_time": "2023-06-12T17:11:35.778866Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "\n",
    "dataframe_1h = pd.read_csv(file_1h[0], parse_dates=['date'])\n",
    "dataframe_1h.set_index('date', inplace=True)\n",
    "\n",
    "dataframe_uqam = pd.read_csv(file_uqam_1h[0], parse_dates=['date'])\n",
    "dataframe_uqam.set_index('date', inplace=True)\n",
    "\n",
    "dataframe_momo = pd.read_csv(file_momo_1h[0], parse_dates=['date'])\n",
    "dataframe_momo.set_index('date', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T17:11:36.535274Z",
     "start_time": "2023-06-12T17:11:35.778892Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Load localisation station\n",
    "lonlat_path = '/upslope/chalifour/projet_maitrise/data_format-master/Data.nosync/Disdrometres_coordonnées.csv'\n",
    "df_disdro = pd.read_csv(lonlat_path, header=0)\n",
    "df_disdro.set_index('Name', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T17:11:36.535451Z",
     "start_time": "2023-06-12T17:11:36.527766Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "phase_list = ['f_0','f_60','f_67','f_69','f_70']\n",
    "phase_list_simulation =['RN','FR','MX','SN']\n",
    "phase_list_simulation_pie =['RN','FR','MX_COLD','MX_HOT','SOL']\n",
    "phase_str = ['None',  'Liquid', 'Freezing rain', 'Mix of Liquid and Solid with $T<0$ \\u2103','Mix of Liquid and Solid with $T\\geq0$ \\u2103','Solid']\n",
    "\n",
    "pType_color = ['k', 'tab:green', 'tab:red','tab:purple','tab:orange' ,'tab:blue',]  # ['firebrick','lime',    'm',       'orange']#"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T17:11:36.549898Z",
     "start_time": "2023-06-12T17:11:36.533107Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "263.0 47.5 353.0 0.0\n"
     ]
    }
   ],
   "source": [
    "m, lonE2p5, latE2p5 = get_proj_extent()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T17:11:37.620587Z",
     "start_time": "2023-06-12T17:11:36.773436Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# load sim dataset station\n",
    "begin,end = '2020-10','2021-07'\n",
    "# 11 km\n",
    "file_11km_stat = glob.glob(os.path.join(data_path + f'data_sim_station/dataset_stat_sim_11km_{begin.strip(\"-\")}_{end.strip(\"-\")}.csv'))\n",
    "\n",
    "dataframe_11km_stat = pd.read_csv(file_11km_stat[0], parse_dates=['time'])\n",
    "dataframe_11km_stat.set_index('time', inplace=True)\n",
    "\n",
    "# uqam et momo 11 km\n",
    "\n",
    "file_11km_Umomo = glob.glob(os.path.join(data_path + f'data_sim_station/dataset_UQAM_MOMO_sim_11km_{begin.strip(\"-\")}_{end.strip(\"-\")}.csv'))\n",
    "dataframe_11km_Umomo = pd.read_csv(file_11km_Umomo[0], parse_dates=['time'])\n",
    "dataframe_11km_Umomo.set_index('time', inplace=True)\n",
    "\n",
    "# 2p5 km\n",
    "file_2p5km_stat = glob.glob(os.path.join(data_path + f'data_sim_station/dataset_stat_sim_2p5km_{begin.strip(\"-\")}_{end.strip(\"-\")}.csv'))\n",
    "dataframe_2p5km_stat = pd.read_csv(file_2p5km_stat[0], parse_dates=['time'])\n",
    "dataframe_2p5km_stat.set_index('time', inplace=True)\n",
    "\n",
    "\n",
    "# uqam et momo 2p5 km\n",
    "\n",
    "file_2p5km_Umomo = glob.glob(os.path.join(data_path + f'data_sim_station/dataset_UQAM_MOMO_sim_2p5km_{begin.strip(\"-\")}_{end.strip(\"-\")}.csv'))\n",
    "dataframe_2p5km_Umomo = pd.read_csv(file_2p5km_Umomo[0], parse_dates=['time'])\n",
    "dataframe_2p5km_Umomo.set_index('time', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T13:53:44.301408Z",
     "start_time": "2023-06-12T13:53:43.727811Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creat mx and sol categorie stat\n",
    "\n",
    "list_df_11km=[]\n",
    "for stat, subdf in dataframe_11km_stat.groupby('filename'):\n",
    "    div = subdf[['RN','SN','PE','FR']]/subdf[['RN','SN','PE','FR']]\n",
    "    sum = div.sum(axis=1)\n",
    "\n",
    "    mask_sum = sum>1\n",
    "\n",
    "    mask_qty = subdf[['RN','SN','PE','FR']]> 0.05\n",
    "    mask_qty = mask_qty.sum(axis=1) > 1\n",
    "\n",
    "\n",
    "    # On utilise les température pour déterminer les mix cold hot\n",
    "    mask_temp_chaud = subdf['TT']>= 0.0\n",
    "    mask_temp_froid = subdf['TT']< 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    subdf.loc[mask_sum & mask_qty,'MX_COLD'] = subdf.loc[mask_sum & mask_qty & mask_temp_froid  ,['RN','SN','PE','FR']].sum(axis=1)\n",
    "    subdf.loc[mask_sum & mask_qty,'MX_HOT'] = subdf.loc[mask_sum & mask_qty & mask_temp_chaud  ,['RN','SN','PE','FR']].sum(axis=1)\n",
    "    subdf.loc[mask_sum & mask_qty & mask_temp_froid ,['RN','SN','PE','FR']] = 0\n",
    "    subdf.loc[mask_sum & mask_qty & mask_temp_chaud ,['RN','SN','PE','FR']] = 0\n",
    "    subdf['SOL'] = subdf[['SN','PE']].sum(axis=1)\n",
    "    subdf.loc[np.isnan(subdf['MX_COLD']),'MX_COLD']=0\n",
    "    subdf.loc[np.isnan(subdf['MX_HOT']),'MX_HOT']=0\n",
    "    subdf.loc[np.isnan(subdf['SOL']),'SOL']=0\n",
    "    list_df_11km.append(subdf)\n",
    "    # dataframe_2p5km_stat.loc[dataframe_2p5km_stat['filename']==stat] = subdf\n",
    "\n",
    "dataframe_11km_stat = pd.concat(list_df_11km)\n",
    "\n",
    "\n",
    "list_df_2p5=[]\n",
    "for stat, subdf in dataframe_2p5km_stat.groupby('filename'):\n",
    "    div = subdf[['RN','SN','PE','FR']]/subdf[['RN','SN','PE','FR']]\n",
    "    sum = div.sum(axis=1)\n",
    "\n",
    "    mask_sum = sum>1\n",
    "\n",
    "    mask_qty = subdf[['RN','SN','PE','FR']]> 0.05\n",
    "    mask_qty = mask_qty.sum(axis=1) > 1\n",
    "\n",
    "\n",
    "    # On utilise les température pour déterminer les mix cold hot\n",
    "    mask_temp_chaud = subdf['TT']>= 0.0\n",
    "    mask_temp_froid = subdf['TT']< 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    subdf.loc[mask_sum & mask_qty,'MX_COLD'] = subdf.loc[mask_sum & mask_qty & mask_temp_froid  ,['RN','SN','PE','FR']].sum(axis=1)\n",
    "    subdf.loc[mask_sum & mask_qty,'MX_HOT'] = subdf.loc[mask_sum & mask_qty & mask_temp_chaud  ,['RN','SN','PE','FR']].sum(axis=1)\n",
    "    subdf.loc[mask_sum & mask_qty & mask_temp_froid ,['RN','SN','PE','FR']] = 0\n",
    "    subdf.loc[mask_sum & mask_qty & mask_temp_chaud ,['RN','SN','PE','FR']] = 0\n",
    "    subdf['SOL'] = subdf[['SN','PE']].sum(axis=1)\n",
    "    subdf.loc[np.isnan(subdf['MX_COLD']),'MX_COLD']=0\n",
    "    subdf.loc[np.isnan(subdf['MX_HOT']),'MX_HOT']=0\n",
    "    subdf.loc[np.isnan(subdf['SOL']),'SOL']=0\n",
    "    list_df_2p5.append(subdf)\n",
    "    # dataframe_2p5km_stat.loc[dataframe_2p5km_stat['filename']==stat] = subdf\n",
    "\n",
    "dataframe_2p5km_stat = pd.concat(list_df_2p5)\n",
    "# uqam et momo\n",
    "list_df_11km=[]\n",
    "for stat, subdf in dataframe_11km_Umomo.groupby('filename'):\n",
    "\n",
    "    div = subdf[['RN','SN','PE','FR']]/subdf[['RN','SN','PE','FR']]\n",
    "    sum = div.sum(axis=1)\n",
    "\n",
    "    mask_sum = sum>1\n",
    "\n",
    "    mask_qty = subdf[['RN','SN','PE','FR']]> 0.05\n",
    "    mask_qty = mask_qty.sum(axis=1) > 1\n",
    "\n",
    "\n",
    "    # On utilise les température pour déterminer les mix cold hot\n",
    "    mask_temp_chaud = subdf['TT']>= 0.0\n",
    "    mask_temp_froid = subdf['TT']< 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    subdf.loc[mask_sum & mask_qty,'MX_COLD'] = subdf.loc[mask_sum & mask_qty & mask_temp_froid  ,['RN','SN','PE','FR']].sum(axis=1)\n",
    "    subdf.loc[mask_sum & mask_qty,'MX_HOT'] = subdf.loc[mask_sum & mask_qty & mask_temp_chaud  ,['RN','SN','PE','FR']].sum(axis=1)\n",
    "    subdf.loc[mask_sum & mask_qty & mask_temp_froid ,['RN','SN','PE','FR']] = 0\n",
    "    subdf.loc[mask_sum & mask_qty & mask_temp_chaud ,['RN','SN','PE','FR']] = 0\n",
    "    subdf['SOL'] = subdf[['SN','PE']].sum(axis=1)\n",
    "    subdf.loc[np.isnan(subdf['MX_COLD']),'MX_COLD']=0\n",
    "    subdf.loc[np.isnan(subdf['MX_HOT']),'MX_HOT']=0\n",
    "    subdf.loc[np.isnan(subdf['SOL']),'SOL']=0\n",
    "    list_df_11km.append(subdf)\n",
    "    # dataframe_2p5km_stat.loc[dataframe_2p5km_stat['filename']==stat] = subdf\n",
    "\n",
    "dataframe_11km_Umomo = pd.concat(list_df_11km)\n",
    "\n",
    "\n",
    "list_df_2p5=[]\n",
    "for stat, subdf in dataframe_2p5km_Umomo.groupby('filename'):\n",
    "    div = subdf[['RN','SN','PE','FR']]/subdf[['RN','SN','PE','FR']]\n",
    "    sum = div.sum(axis=1)\n",
    "\n",
    "    mask_sum = sum>1\n",
    "\n",
    "    mask_qty = subdf[['RN','SN','PE','FR']]> 0.05\n",
    "    mask_qty = mask_qty.sum(axis=1) > 1\n",
    "\n",
    "\n",
    "    # On utilise les température pour déterminer les mix cold hot\n",
    "    mask_temp_chaud = subdf['TT']>= 0.0\n",
    "    mask_temp_froid = subdf['TT']< 0.0\n",
    "\n",
    "    subdf.loc[mask_sum & mask_qty,'MX_COLD'] = subdf.loc[mask_sum & mask_qty & mask_temp_froid  ,['RN','SN','PE','FR']].sum(axis=1)\n",
    "    subdf.loc[mask_sum & mask_qty,'MX_HOT'] = subdf.loc[mask_sum & mask_qty & mask_temp_chaud  ,['RN','SN','PE','FR']].sum(axis=1)\n",
    "    subdf.loc[mask_sum & mask_qty & mask_temp_froid ,['RN','SN','PE','FR']] = 0\n",
    "    subdf.loc[mask_sum & mask_qty & mask_temp_chaud ,['RN','SN','PE','FR']] = 0\n",
    "    subdf['SOL'] = subdf[['SN','PE']].sum(axis=1)\n",
    "    subdf.loc[np.isnan(subdf['MX_COLD']),'MX_COLD']=0\n",
    "    subdf.loc[np.isnan(subdf['MX_HOT']),'MX_HOT']=0\n",
    "    subdf.loc[np.isnan(subdf['SOL']),'SOL']=0\n",
    "\n",
    "    list_df_2p5.append(subdf)\n",
    "    # dataframe_2p5km_stat.loc[dataframe_2p5km_stat['filename']==stat] = subdf\n",
    "\n",
    "dataframe_2p5km_Umomo = pd.concat(list_df_2p5)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pie chart for each available station on the map\n",
    "image_output_dpi = 200\n",
    "fig = plt.figure(facecolor='white', figsize=(8 * 1.5, 3 *3.5))\n",
    "\n",
    "spec = fig.add_gridspec(ncols=3, nrows=1,)\n",
    "ax0 = fig.add_subplot(spec[0,0], projection=ccrs.LambertConformal())\n",
    "ax1 = fig.add_subplot(spec[0, 1], projection=ccrs.LambertConformal())\n",
    "ax2 = fig.add_subplot(spec[0, 2], projection=ccrs.LambertConformal())\n",
    "\n",
    "list_axs=[ax0,ax1,ax2]\n",
    "\n",
    "xll, yll = m.transform_point(lonE2p5[0, 0], latE2p5[0, 0], ccrs.PlateCarree())\n",
    "xur, yur = m.transform_point(lonE2p5[-1, -1], latE2p5[-1, -1], ccrs.PlateCarree())\n",
    "\n",
    "xm, ym = m.transform_point(-73.6053330201976, 45.52028815987258, ccrs.PlateCarree())\n",
    "\n",
    "for ax_map in list_axs:\n",
    "    # Set geographic features\n",
    "    ax_map.add_feature(cfeature.OCEAN.with_scale('50m'), alpha=0.5)  # couche ocean\n",
    "    ax_map.add_feature(cfeature.LAND.with_scale('50m'), facecolor='none')  # couche land\n",
    "    # ax.add_feature(cfeature.LAKES.with_scale('50m'))      # couche lac\n",
    "    ax_map.add_feature(cfeature.BORDERS.with_scale('50m'))  # couche frontieres\n",
    "    # ax.add_feature(cfeature.RIVERS.with_scale('50m'))     # couche rivières\n",
    "    coast = cfeature.NaturalEarthFeature(category='physical', scale='10m', facecolor='none',\n",
    "                                         name='coastline')  # Couche côtières\n",
    "    ax_map.add_feature(coast, edgecolor='black')\n",
    "    ax_map.add_feature(cfeature.LAKES)\n",
    "    states_provinces = cfeature.NaturalEarthFeature(category='cultural', name='admin_1_states_provinces_lines',\n",
    "                                                    scale='10m', facecolor='none')  # Couche provinces\n",
    "    ax_map.add_feature(states_provinces, edgecolor='grey')\n",
    "\n",
    "# To help the layout of the figure after saving\n",
    "list_stat_prob = ['GAREMANG','PIPMUA_G']\n",
    "\n",
    "time_range_hours = pd.date_range(begin,end.replace(\"7\",\"6\"),freq='H',closed='right')[:-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for station, subdf in dataframe_1h.groupby('filename'):\n",
    "\n",
    "    subdf = subdf.loc[begin:end.replace('7','5')]\n",
    "\n",
    "\n",
    "    if len(subdf.index)!=0:\n",
    "        if subdf['#nan_1h'].sum()/(4*len(time_range_hours))*100 < 10 and station not in list_stat_prob:\n",
    "\n",
    "            # obs\n",
    "            lon = df_disdro.loc[station].X\n",
    "            lat = df_disdro.loc[station].Y\n",
    "\n",
    "            list_count = []\n",
    "            for i, num in enumerate(phase_list[1:]):\n",
    "\n",
    "                if num == 'f_69':\n",
    "                    mask_temp_froid = subdf['temp_moy']< 0.0\n",
    "                    list_count.append(subdf.loc[mask_temp_froid,num].sum())\n",
    "\n",
    "                    mask_temp_chaud = subdf['temp_moy']>= 0.0\n",
    "                    list_count.append(subdf.loc[mask_temp_chaud,num].sum())\n",
    "\n",
    "                else:\n",
    "                    list_count.append(subdf[num].sum())\n",
    "            list_count = np.array(list_count)\n",
    "\n",
    "\n",
    "            if np.sum(list_count) != 0:\n",
    "                lon_1, lat_1 = ccrs.LambertConformal().transform_point(lon, lat, ccrs.PlateCarree())\n",
    "\n",
    "                ax_sub = inset_axes(ax0, width=0.4, height=0.4, loc=10,\n",
    "                                    bbox_to_anchor=(lon_1, lat_1),\n",
    "                                    bbox_transform=ax0.transData,\n",
    "                                    borderpad=0)\n",
    "\n",
    "                wedges, texts = ax_sub.pie(list_count, center=(lon, lat), colors=pType_color[1:], radius=0.7,\n",
    "                                           wedgeprops={'linewidth': 0.5, \"edgecolor\": \"k\"})\n",
    "                ax_sub.set_aspect(\"equal\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                list_stat_prob.append(station)\n",
    "#  11 km\n",
    "for station, subdf_11km in dataframe_11km_stat.groupby('filename'):\n",
    "    # nb de nan plus grand que 10% du dataframe\n",
    "    subdf_stat = dataframe_1h.loc[dataframe_1h['filename']==station]\n",
    "    subdf_stat = subdf_stat.loc[begin:end.replace('7','5')]\n",
    "\n",
    "    if len(subdf.index)!=0:\n",
    "        if subdf_stat['#nan_1h'].sum()/(4*len(time_range_hours))*100 < 10 and station not in list_stat_prob:\n",
    "\n",
    "            # obs\n",
    "            lon = df_disdro.loc[station].X\n",
    "            lat = df_disdro.loc[station].Y\n",
    "\n",
    "            list_count = []\n",
    "            for i, num in enumerate(phase_list_simulation_pie):\n",
    "                subdf_11km = subdf_11km.loc[begin:end]\n",
    "                list_count.append(subdf_11km[num].sum())\n",
    "            list_count = np.array(list_count)\n",
    "\n",
    "            if np.sum(list_count) != 0:\n",
    "                lon_1, lat_1 = ccrs.LambertConformal().transform_point(lon, lat, ccrs.PlateCarree())\n",
    "\n",
    "                ax_sub = inset_axes(ax1, width=0.4, height=0.4, loc=10,\n",
    "                                    bbox_to_anchor=(lon_1, lat_1),\n",
    "                                    bbox_transform=ax1.transData,\n",
    "                                    borderpad=0)\n",
    "\n",
    "                wedges, texts = ax_sub.pie(list_count, center=(lon, lat), colors=pType_color[1:], radius=0.7,\n",
    "                                           wedgeprops={'linewidth': 0.5, \"edgecolor\": \"k\"})\n",
    "                ax_sub.set_aspect(\"equal\")\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "# 2p5 km\n",
    "\n",
    "for station, subdf_2p5km in dataframe_2p5km_stat.groupby('filename'):\n",
    "    # nb de nan plus grand que 10% du dataframe\n",
    "    subdf_stat = dataframe_1h.loc[dataframe_1h['filename']==station]\n",
    "    subdf_stat = subdf_stat.loc[begin:end.replace('7','5')]\n",
    "\n",
    "    if len(subdf.index)!=0:\n",
    "        if subdf_stat['#nan_1h'].sum()/(4*len(time_range_hours))*100 < 10 and station not in list_stat_prob:\n",
    "\n",
    "            # obs\n",
    "            lon = df_disdro.loc[station].X\n",
    "            lat = df_disdro.loc[station].Y\n",
    "\n",
    "            list_count = []\n",
    "            for i, num in enumerate(phase_list_simulation_pie):\n",
    "                subdf_2p5km = subdf_2p5km.loc[begin:end]\n",
    "                list_count.append(subdf_2p5km[num].sum())\n",
    "            list_count = np.array(list_count)\n",
    "\n",
    "            if np.sum(list_count) != 0:\n",
    "                lon_1, lat_1 = ccrs.LambertConformal().transform_point(lon, lat, ccrs.PlateCarree())\n",
    "\n",
    "                ax_sub = inset_axes(ax2, width=0.4, height=0.4, loc=10,\n",
    "                                    bbox_to_anchor=(lon_1, lat_1),\n",
    "                                    bbox_transform=ax2.transData,\n",
    "                                    borderpad=0)\n",
    "\n",
    "                wedges, texts = ax_sub.pie(list_count, center=(lon, lat), colors=pType_color[1:], radius=0.7,\n",
    "                                           wedgeprops={'linewidth': 0.5, \"edgecolor\": \"k\"})\n",
    "                ax_sub.set_aspect(\"equal\")\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "# uqam\n",
    "name_uqam_momo = ['UQAM_PK','NEIGE']\n",
    "lat_uqam_momo = [45.508594,47.322437368331876]\n",
    "lon_uqam_momo = [-73.568741,-71.14730110000002]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_count_uqam = []\n",
    "for i, num in enumerate(phase_list[1:]):\n",
    "    dataframe_uqam_ph = dataframe_uqam.loc[begin:end]\n",
    "    if num == 'f_69':\n",
    "        mask_temp_froid = dataframe_uqam_ph['temp_moy']< 0.0\n",
    "        try:\n",
    "            list_count_uqam.append(dataframe_uqam_ph.loc[mask_temp_froid,num].sum())\n",
    "        except:\n",
    "            list_count_uqam.append(0)\n",
    "\n",
    "        mask_temp_chaud = dataframe_uqam_ph['temp_moy']>= 0.0\n",
    "\n",
    "        try:\n",
    "            list_count_uqam.append(dataframe_uqam_ph.loc[mask_temp_chaud,num].sum())\n",
    "        except:\n",
    "            list_count_uqam.append(0)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            list_count_uqam.append(dataframe_uqam_ph[num].sum())\n",
    "        except:\n",
    "            list_count_uqam.append(0)\n",
    "list_count_uqam = np.array(list_count_uqam)\n",
    "\n",
    "if np.sum(list_count_uqam) != 0:\n",
    "    lon_uqam_1, lat_uqam_1 = ccrs.LambertConformal().transform_point(lon_uqam_momo[0], lat_uqam_momo[0], ccrs.PlateCarree())\n",
    "\n",
    "    ax_sub = inset_axes(ax0, width=0.4, height=0.4, loc=10,\n",
    "                        bbox_to_anchor=(lon_uqam_1, lat_uqam_1),\n",
    "                        bbox_transform=ax0.transData,\n",
    "                        borderpad=0)\n",
    "\n",
    "    wedges, texts = ax_sub.pie(list_count_uqam, center=(lon_uqam_momo[0], lat_uqam_momo[0]), colors=pType_color[1:], radius=0.7,\n",
    "                               wedgeprops={'linewidth': 0.5, \"edgecolor\": \"k\"})\n",
    "    ax_sub.set_aspect(\"equal\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_count_momo = []\n",
    "for i, num in enumerate(phase_list[1:]):\n",
    "    dataframe_momo_ph = dataframe_momo.loc[begin:end]\n",
    "    if num == 'f_69':\n",
    "        mask_temp_froid = dataframe_momo_ph['temp_moy']< 0.0\n",
    "        try:\n",
    "            list_count_momo.append(dataframe_momo_ph.loc[mask_temp_froid,num].sum())\n",
    "        except:\n",
    "            list_count_momo.append(0)\n",
    "\n",
    "        mask_temp_chaud = dataframe_momo_ph['temp_moy']>= 0.0\n",
    "        try:\n",
    "            list_count_momo.append(dataframe_momo_ph.loc[mask_temp_chaud,num].sum())\n",
    "        except:\n",
    "            list_count_momo.append(0)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            list_count_momo.append(dataframe_momo_ph[num].sum())\n",
    "        except:\n",
    "            list_count_momo.append(0)\n",
    "\n",
    "\n",
    "list_count_momo = np.array(list_count_momo)\n",
    "\n",
    "if np.sum(list_count_momo) != 0:\n",
    "    lon_momo_1, lat_momo_1 = ccrs.LambertConformal().transform_point(lon_uqam_momo[1], lat_uqam_momo[1], ccrs.PlateCarree())\n",
    "\n",
    "    ax_sub = inset_axes(ax0, width=0.4, height=0.4, loc=10,\n",
    "                        bbox_to_anchor=(lon_momo_1, lat_momo_1),\n",
    "                        bbox_transform=ax0.transData,\n",
    "                        borderpad=0)\n",
    "\n",
    "    wedges, texts = ax_sub.pie(list_count_momo, center=(lon_uqam_momo[1], lat_uqam_momo[1]), colors=pType_color[1:], radius=0.7,\n",
    "                               wedgeprops={'linewidth': 0.5, \"edgecolor\": \"k\"})\n",
    "    ax_sub.set_aspect(\"equal\")\n",
    "\n",
    "\n",
    "# uqam et momo 11km\n",
    "for idx_umomo_11km,stat_umomo in enumerate(name_uqam_momo) :\n",
    "    subdf_11km_uqammomo = dataframe_11km_Umomo.loc[dataframe_11km_Umomo['filename'] == stat_umomo]\n",
    "\n",
    "    list_count_11km = []\n",
    "    for i, num in enumerate(phase_list_simulation_pie):\n",
    "\n",
    "        list_count_11km.append(subdf_11km_uqammomo[num].sum())\n",
    "\n",
    "    list_count_uqam = np.array(list_count_11km)\n",
    "\n",
    "    if np.sum(list_count_11km) != 0:\n",
    "        lon_uqam_1, lat_uqam_1 = ccrs.LambertConformal().transform_point(lon_uqam_momo[idx_umomo_11km], lat_uqam_momo[idx_umomo_11km], ccrs.PlateCarree())\n",
    "\n",
    "        ax_sub = inset_axes(ax1, width=0.4, height=0.4, loc=10,\n",
    "                            bbox_to_anchor=(lon_uqam_1, lat_uqam_1),\n",
    "                            bbox_transform=ax1.transData,\n",
    "                            borderpad=0)\n",
    "\n",
    "        wedges, texts = ax_sub.pie(list_count_uqam, center=(lon_uqam_momo[idx_umomo_11km], lat_uqam_momo[idx_umomo_11km]), colors=pType_color[1:], radius=0.7,\n",
    "                                   wedgeprops={'linewidth': 0.5, \"edgecolor\": \"k\"})\n",
    "        ax_sub.set_aspect(\"equal\")\n",
    "\n",
    "# uqam et momo 2p5km\n",
    "for idx_umomo_2p5km,stat_umomo in enumerate(name_uqam_momo) :\n",
    "    subdf_2p5km_uqammomo = dataframe_2p5km_Umomo.loc[dataframe_2p5km_Umomo['filename'] == stat_umomo]\n",
    "    list_count_2p5km = []\n",
    "    for i, num in enumerate(phase_list_simulation_pie):\n",
    "\n",
    "        list_count_2p5km.append(subdf_2p5km_uqammomo[num].sum())\n",
    "\n",
    "    list_count_uqam = np.array(list_count_2p5km)\n",
    "\n",
    "    if np.sum(list_count_uqam) != 0:\n",
    "        lon_uqam_1, lat_uqam_1 = ccrs.LambertConformal().transform_point(lon_uqam_momo[idx_umomo_2p5km],\n",
    "                                                                         lat_uqam_momo[idx_umomo_2p5km], ccrs.PlateCarree())\n",
    "\n",
    "        ax_sub = inset_axes(ax2, width=0.4, height=0.4, loc=10,\n",
    "                            bbox_to_anchor=(lon_uqam_1, lat_uqam_1),\n",
    "                            bbox_transform=ax2.transData,\n",
    "                            borderpad=0)\n",
    "\n",
    "        wedges, texts = ax_sub.pie(list_count_uqam, center=(lon_uqam_momo[idx_umomo_2p5km], lat_uqam_momo[idx_umomo_2p5km]), colors=pType_color[1:], radius=0.7,\n",
    "                                   wedgeprops={'linewidth': 0.5, \"edgecolor\": \"k\"})\n",
    "        ax_sub.set_aspect(\"equal\")\n",
    "\n",
    "for ax_aff in list_axs:\n",
    "    ax_aff.set_extent([xll + 17, xur - 9, yll + 4, yur - 1], crs=m)\n",
    "\n",
    "time_text = f'{subdf.index[0].strftime(\"%Y-%m-%d\")} to {subdf.index[-1].strftime(\"%Y-%m-%d\")}'\n",
    "\n",
    "ax2.annotate(f'{time_text}', xy=(0.15, 1.01), xycoords='axes fraction', fontsize=14)\n",
    "# title\n",
    "bbox = dict(boxstyle=\"square\", fc=\"w\")\n",
    "ax0.annotate(f'(a) Observation', xy=(0.03, 0.93), bbox=bbox, xycoords='axes fraction', fontsize=12)\n",
    "ax1.annotate(f'(b) CORDEX 0.11\\u00b0', xy=(0.03, 0.93),bbox=bbox,xycoords='axes fraction', fontsize=12)\n",
    "ax2.annotate(f'(c) Extended East 0.0225\\u00b0', xy=(0.03, 0.93), bbox=bbox, xycoords='axes fraction', fontsize=12)\n",
    "handles = [Patch(facecolor=color, label=label, edgecolor=\"k\") for label, color in zip(phase_str[1:], pType_color[1:])]\n",
    "\n",
    "plt.rcParams['legend.handlelength'] = 1\n",
    "\n",
    "ax1.legend(handles=handles, loc='upper center', bbox_to_anchor=(0.5, -0.01),\n",
    "          fancybox=True, shadow=True, ncol=2, fontsize=14)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "image_savepath = fr\"/upslope/chalifour/projet_maitrise/fig/carte_stat_pie/map_pie_{subdf.index[0].strftime('%Y')}_{subdf.index[-1].strftime('%Y')}.png\"\n",
    "fig.savefig(image_savepath, dpi=image_output_dpi, format='png', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# path simulation 11 km\n",
    "# path_sim_11km_1 = glob.glob(\"/BIG1/roberge/Output/GEM5/Cascades_CORDEX/CLASS/Safe_versions/Spinup/NAM-11m_ERA5_GEM5_CLASS_NV_NA_newP3-SCPF_SN8_20yrs/Samples/NAM-11m_ERA5_GEM5_CLASS_NV_NA_newP3-SCPF_SN8_20yrs_20220*[0-5]\")\n",
    "# path_sim_11km_2 = glob.glob(\"/BIG1/roberge/Output/GEM5/Cascades_CORDEX/CLASS/Safe_versions/Spinup/NAM-11m_ERA5_GEM5_CLASS_NV_NA_newP3-SCPF_SN8_20yrs/Samples/NAM-11m_ERA5_GEM5_CLASS_NV_NA_newP3-SCPF_SN8_20yrs_20211*[0-2]\")\n",
    "# path_sim_11km = sorted(path_sim_11km_1+path_sim_11km_2)\n",
    "\n",
    "# path sim 11km avec p3\n",
    "path_sim_11km_1 = glob.glob(\"/chinook/roberge/Output/GEM5/Olivier/NAM-11m_ERA5_GEM50_PCPTYPEnil/Samples/NAM-11m_ERA5_GEM50_PCPTYPEnil_20210*[0-5]\")\n",
    "\n",
    "path_sim_11km_2 = glob.glob(\"/chinook/roberge/Output/GEM5/Olivier/NAM-11m_ERA5_GEM50_PCPTYPEnil/Samples/NAM-11m_ERA5_GEM50_PCPTYPEnil_20201*[0-2]\")\n",
    "path_sim_11km = sorted(path_sim_11km_1+path_sim_11km_2)\n",
    "\n",
    "\n",
    "# path simulation 2.5 km\n",
    "# path_sim_2p5_1 = glob.glob(\"/pampa/roberge/Output/GEM5/Cascades_CORDEX/CLASS/Safe_versions/Spinup/ECan_2.5km_NAM11mP3_newP3_CLASS_DEEPoff_SHALon/Samples/ECan_2.5km_NAM11mP3_newP3_CLASS_DEEPoff_SHALon_20210*[0-5]\")\n",
    "# path_sim_2p5_2 = glob.glob(\"/pampa/roberge/Output/GEM5/Cascades_CORDEX/CLASS/Safe_versions/Spinup/ECan_2.5km_NAM11mP3_newP3_CLASS_DEEPoff_SHALon/Samples/ECan_2.5km_NAM11mP3_newP3_CLASS_DEEPoff_SHALon_20201*[0-2]\")\n",
    "# path_sim_2p5km = sorted(path_sim_2p5_1+path_sim_2p5_2)\n",
    "\n",
    "path_sim_2p5_1 = glob.glob(\"/chinook/roberge/Output/GEM5/Olivier/ECan_2.5km_NAM11mP3_GEM50_PCPTYPEnil/Samples/ECan_2.5km_NAM11mP3_GEM50_PCPTYPEnil_20210*[0-5]\")\n",
    "path_sim_2p5_2 = glob.glob(\"/chinook/roberge/Output/GEM5/Olivier/ECan_2.5km_NAM11mP3_GEM50_PCPTYPEnil/Samples/ECan_2.5km_NAM11mP3_GEM50_PCPTYPEnil_20201*[0-2]\")\n",
    "path_sim_2p5km = sorted(path_sim_2p5_1+path_sim_2p5_2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T15:30:34.050306Z",
     "start_time": "2023-06-12T15:30:33.996988Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting 2020-10 \n",
      "getting HR\n",
      "getting HU\n",
      "Getting 2020-11 \n",
      "getting HR\n",
      "getting HU\n",
      "Getting 2020-12 \n",
      "getting HR\n",
      "getting HU\n",
      "Getting 2021-1 \n",
      "getting HR\n",
      "getting HU\n",
      "Getting 2021-2 \n",
      "getting HR\n",
      "getting HU\n",
      "Getting 2021-3 \n",
      "getting HR\n",
      "getting HU\n",
      "Getting 2021-4 \n",
      "getting HR\n",
      "getting HU\n",
      "Getting 2021-5 \n",
      "getting HR\n",
      "getting HU\n"
     ]
    }
   ],
   "source": [
    "# path_sim_2p5_2elev_dict_umomo = {'UQAM_PK': 70,\n",
    "#                    'NEIGE': 664 }\n",
    "\n",
    "# load simulation data Uqam et momo\n",
    "# phase_list_simulation =['RN','SN','FR','PE']\n",
    "# var_list_simulation =['UU','VV','TT','PR','RN','SN','FR','PE',]\n",
    "var_list_simulation =['HR','HU']\n",
    "phase_name_simulation  = ['Rain','Snow','Freezing\\nRain','Refrozen\\nprecipitation']\n",
    "\n",
    "name_uqam_momo = ['UQAM_PK','NEIGE']\n",
    "lat_uqam_momo = [45.508594,47.322437368331876]\n",
    "lon_uqam_momo = [-73.568741,-71.14730110000002]\n",
    "\n",
    "begin,end = '2020-10','2021-07'\n",
    "timerange_month = pd.date_range(begin,end,freq='m',closed='right')\n",
    "\n",
    "dict_idx_nearest_pt={'UQAM_PK':[],'NEIGE':[]}\n",
    "\n",
    "\n",
    "# df_sim_stat = pd.DataFrame()\n",
    "dict_gen = {'time':[],'filename':[]}\n",
    "for phase in var_list_simulation:\n",
    "    dict_gen[phase]=[]\n",
    "\n",
    "for idx in range(len(timerange_month.month)-1):\n",
    "    print(f'Getting {timerange_month[idx].year}-{timerange_month[idx].month} ')\n",
    "\n",
    "    timerange_hour = pd.date_range(timerange_month[idx].strftime('%Y-%m'),timerange_month[idx+1].strftime('%Y-%m'),freq='1h')\n",
    "\n",
    "    timerange_hour = timerange_hour[timerange_hour.month == timerange_month[idx].month]\n",
    "\n",
    "    timerange_hour_datev = []\n",
    "\n",
    "    for date_h in timerange_hour:\n",
    "        timerange_hour_datev.append(rpn_chris.date_to_datev(date_h))\n",
    "    try:\n",
    "        rmn.fstcloseall(fid_2)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # fid_2 = rmn.fstopenall(path_sim_11km[idx],rmn.FST_RO)\n",
    "    fid_2 = rmn.fstopenall(path_sim_2p5km[idx],rmn.FST_RO)\n",
    "\n",
    "    for j,phase in enumerate(var_list_simulation):\n",
    "        print(f'getting {phase}')\n",
    "        for hidx,datev in enumerate(timerange_hour_datev):\n",
    "\n",
    "            # key1 = rmn.fstinf(fid, nomvar=phase)\n",
    "            if phase in ['TT']:\n",
    "                ip1_1_5m = rmn.ip1_val(1.5, rmn.LEVEL_KIND_MGL)\n",
    "                rec = rmn.fstlir(fid_2, nomvar=phase, datev=datev,ip1=ip1_1_5m)\n",
    "            elif phase in ['UU','VV']:\n",
    "                ip1_10m = rmn.ip1_val(10, rmn.LEVEL_KIND_MGL)\n",
    "                rec = rmn.fstlir(fid_2, nomvar=phase, datev=datev,ip1=ip1_10m)\n",
    "            else:\n",
    "                rec = rmn.fstlir(fid_2, nomvar=phase, datev=datev)\n",
    "\n",
    "\n",
    "            for idx_stat in range(len(name_uqam_momo)):\n",
    "\n",
    "                lon_stat_2 = lon_uqam_momo[idx_stat]\n",
    "                lat_stat_2 = lat_uqam_momo[idx_stat]\n",
    "                station = name_uqam_momo[idx_stat]\n",
    "\n",
    "                if hidx ==0:\n",
    "                    # dict_gen[phase].append(np.zeros((6,6)))\n",
    "                    dict_gen[phase].append(0)\n",
    "                    if j == 0:\n",
    "                        dict_gen['time'].append(timerange_hour[hidx])\n",
    "                        dict_gen['filename'].append(name_uqam_momo[idx_stat])\n",
    "                else:\n",
    "\n",
    "                    mygrid = rmn.readGrid(fid_2,rec)              # Get the grid information for the (LAM) Grid -- Reads the tictac's\n",
    "                    latlondict = rmn.gdll(mygrid)               # Create 2-D lat and lon fields from the grid information\n",
    "                    lat_var = latlondict['lat']                     # Assign 'lat' to 2-D latitude field\n",
    "                    lon_var = latlondict['lon']\n",
    "\n",
    "                    # Assign 'lon' to 2-D longitude field\n",
    "                    # Close the RPN file\n",
    "\n",
    "                    if len(dict_idx_nearest_pt[station]) == 0:\n",
    "                        # path_elev_11km = \"/BIG1/roberge/Output/GEM5/Cascades_CORDEX/CLASS/Safe_versions/Spinup/NAM-11m_ERA5_GEM5_CLASS_NV_NA_newP3-SCPF_SN8_20yrs/Samples/NAM-11m_ERA5_GEM5_CLASS_NV_NA_newP3-SCPF_SN8_20yrs_step0/dm2000010100_00000000p\"\n",
    "                        # path_elev_2p5km = \"/pampa/roberge/Output/GEM5/Cascades_CORDEX/CLASS/Safe_versions/Spinup/ECan_2.5km_NAM11mP3_newP3_CLASS_DEEPoff_SHALon/Analysis/ECan_2.5km_NAM11mP3_newP3_CLASS_DEEPoff_SHALon_20150901-00000000\"\n",
    "\n",
    "                        # fid_elev = rmn.fstopenall(path_elev_11km,rmn.FST_RO)\n",
    "                        # rec_elev = rmn.fstlir(fid_elev, nomvar='ME')\n",
    "                        # mygrid_elev = rmn.readGrid(fid_elev,rec_elev)\n",
    "                        # latlondict_elev = rmn.gdll(mygrid_elev)\n",
    "\n",
    "                        # pyt = (lon_var-(360+lon_stat_2))**2+(lat_var-lat_stat_2)**2+(rec_elev['d']-elev_dict_umomo[name_uqam_momo[idx_stat]])**2\n",
    "                        pyt = (lon_var-(360+lon_stat_2))**2+(lat_var-lat_stat_2)**2\n",
    "\n",
    "                        idx_lat,idx_lon = np.unravel_index(np.nanargmin(pyt), pyt.shape)\n",
    "                        dict_idx_nearest_pt[station] = [idx_lat,idx_lon]\n",
    "\n",
    "                        var_pt = [rec['d'][idx_lat,idx_lon]]\n",
    "                        # var_pt = [rec['d'][idx_lat-3:idx_lat+3,idx_lon-3:idx_lon+3]]\n",
    "                        # lon_pt_array = lon_var[idx_lat-3:idx_lat+3,idx_lon-3:idx_lon+3]\n",
    "                        # lat_pt_array = lat_var[idx_lat-3:idx_lat+3,idx_lon-3:idx_lon+3]\n",
    "                        # np.save(f'//upslope/chalifour/projet_maitrise/data_sim_station/closest_point_array/lon_11km_{station}',lon_pt_array)\n",
    "                        # np.save(f'//upslope/chalifour/projet_maitrise/data_sim_station/closest_point_array/lat_11km_{station}',lat_pt_array)\n",
    "\n",
    "                    else:\n",
    "                        idx_lat,idx_lon = dict_idx_nearest_pt[station]\n",
    "                        var_pt=[rec['d'][idx_lat,idx_lon]]\n",
    "                        # var_pt = [rec['d'][idx_lat-3:idx_lat+3,idx_lon-3:idx_lon+3]]\n",
    "\n",
    "                    # var_pt = rmn.gdllsval(latlondict, [lat_stat_2], [lon_stat_2], rec['d']) #Get value of var interpolated to lat/lon point\n",
    "\n",
    "                    if phase == 'TT':\n",
    "                        dict_gen[phase].append(var_pt[0])\n",
    "                    elif phase in ['UU','VV']:\n",
    "                        dict_gen[phase].append(var_pt[0]*0.514444)\n",
    "                    elif phase in ['PR','RN','SN','FR','PE']:\n",
    "                        dict_gen[phase].append(var_pt[0]* 1000* 3600)\n",
    "                    else:\n",
    "                        dict_gen[phase].append(var_pt[0])\n",
    "\n",
    "                    if j == 0:\n",
    "                        dict_gen['time'].append(timerange_hour[hidx])\n",
    "                        dict_gen['filename'].append(name_uqam_momo[idx_stat])\n",
    "    rmn.fstcloseall(fid_2)\n",
    "\n",
    "df_sim_stat = pd.DataFrame.from_dict(dict_gen)\n",
    "\n",
    "df_sim_stat.set_index('time',inplace=True)\n",
    "df_sim_stat.sort_values(['filename','time'],inplace=True)\n",
    "\n",
    "# df_sim_stat.to_csv(f'//upslope/chalifour/projet_maitrise/data_sim_station/dataset_UQAM_MOMO_sim_11km_{begin.strip(\"_\")}_{end.strip(\"_\")}_HR.csv')\n",
    "df_sim_stat.to_csv(f'//upslope/chalifour/projet_maitrise/data_sim_station/closest_point/dataset_UQAM_MOMO_sim_2p5kmP3_{begin.strip(\"_\")}_{end.strip(\"_\")}_HR.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T15:38:57.488617Z",
     "start_time": "2023-06-12T15:30:34.512086Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# elev_dict = {   'ARGENT': 641,\n",
    "#                 'AUXLOUPS': 537.9,\n",
    "#                 'BAUBERT': 541,\n",
    "#                 'BETSIA_M': 403, # à valider\n",
    "#                 'CABITUQG': 491,\n",
    "#                 'LCABITUQ':491.0,\n",
    "#                 'CONRAD': 433,\n",
    "#                 'DIAMAND': 373,\n",
    "#                 'GAREMANG': 778, # à valider\n",
    "#                 'HARTJ_G': 460,\n",
    "#                 'LACROI_G': 621,\n",
    "#                 'LAFLAM_G': 519,\n",
    "#                 'LAVAL': np.nan,\n",
    "#                 'LBARDO_G': 486,\n",
    "#                 'LEVASSEU': 466,\n",
    "#                 'LOUISE_G': 397,\n",
    "#                 'LOUIS': 315,\n",
    "#                 'MOUCHA_M': 565,\n",
    "#                 'NOIRS':385,\n",
    "#                 'PARENT_G': 442,\n",
    "#                 'PARLEUR': 485,\n",
    "#                 'PERDRIX': 315,\n",
    "#                 'PIPMUA_G': 566.2,\n",
    "#                 'PORTO': 413,\n",
    "#                 'ROMA_SE': 104,\n",
    "#                 'ROUSSY_G': 456,\n",
    "#                 'RTOULNUS': 688,\n",
    "#                 'SM3CAM_G': 522,\n",
    "#                 'STMARG_G': 461,\n",
    "#                 'SAUTEREL':459 ,\n",
    "#                 'WABISTAN': 565,\n",
    "#                 'WEYMOU_G': 400,\n",
    "#                 'NA':np.nan,\n",
    "#                     }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T15:49:43.525890Z",
     "start_time": "2023-06-12T15:49:43.475104Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/361646 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "236b27c628e9407da451b2ad041f946a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/361646 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f03cb25caea74cc6bab2996d60d39a26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load simulation data\n",
    "\n",
    "# phase_list_simulation =['RN','SN','FR','PE']\n",
    "# var_list_simulation =['TT','PR','RN','SN','FR','PE']\n",
    "var_list_simulation =['HR','HU']\n",
    "phase_name_simulation  = ['Rain','Snow','Freezing\\nRain','Refrozen\\nprecipitation']\n",
    "# for begin,end in zip(['2021-10'],['2022-07']):\n",
    "for begin,end in zip(['2020-10','2021-10'],['2021-07','2022-07']):\n",
    "\n",
    "    # path_sim_1 = glob.glob(f\"/chinook/roberge/Output/GEM5/Olivier/NAM-11m_ERA5_GEM50_PCPTYPEnil/Samples/NAM-11m_ERA5_GEM50_PCPTYPEnil_{end[0:4]}0*[0-5]\")\n",
    "    # path_sim_2 = glob.glob(f\"/chinook/roberge/Output/GEM5/Olivier/NAM-11m_ERA5_GEM50_PCPTYPEnil/Samples/NAM-11m_ERA5_GEM50_PCPTYPEnil_{begin[0:4]}1*[0-2]\")\n",
    "\n",
    "    path_sim_1 = glob.glob(f\"/chinook/roberge/Output/GEM5/Olivier/ECan_2.5km_NAM11mP3_GEM50_PCPTYPEnil/Samples/ECan_2.5km_NAM11mP3_GEM50_PCPTYPEnil_{end[0:4]}0*[0-5]\")\n",
    "    path_sim_2 = glob.glob(f\"/chinook/roberge/Output/GEM5/Olivier/ECan_2.5km_NAM11mP3_GEM50_PCPTYPEnil/Samples/ECan_2.5km_NAM11mP3_GEM50_PCPTYPEnil_{begin[0:4]}1*[0-2]\")\n",
    "    #\n",
    "    path_sim_2p5km = sorted(path_sim_1+path_sim_2)\n",
    "\n",
    "    # begin,end = '2021-10','2022-07'\n",
    "    timerange_month = pd.date_range(begin,end,freq='m',closed='right')\n",
    "\n",
    "    dict_idx_nearest_pt={}\n",
    "    for station, subdf in dataframe_1h.groupby('filename'):\n",
    "        subdf = subdf.loc[begin:end]\n",
    "        if len(subdf.index)!=0:\n",
    "            if subdf['#nan_1h'].sum()/(4*len(subdf.index))*100 < 10 or station is not ['GAREMANG','PIPMUA_G']:\n",
    "                dict_idx_nearest_pt[station]=[]\n",
    "\n",
    "\n",
    "    # df_sim_stat = pd.DataFrame()\n",
    "    dict_gen = {'time':[],'filename':[]}\n",
    "    for phase in var_list_simulation:\n",
    "        dict_gen[phase]=[]\n",
    "    timerange_hour_b = pd.date_range(timerange_month[0].strftime('%Y-%m'),timerange_month[len(timerange_month.month)-1].strftime('%Y-%m'),freq='1h')\n",
    "    list_stat = [i for i,subdf in dataframe_1h.groupby('filename')]\n",
    "    with tqdm(total=len(var_list_simulation) * len(timerange_hour_b) * len(list_stat)) as pbar:\n",
    "\n",
    "        for idx in range(len(timerange_month.month)-1):\n",
    "            # idx=timerange_month.month[-1]+1\n",
    "\n",
    "            # print(f'Getting {timerange_month[idx].year}-{timerange_month[idx].month} ')\n",
    "\n",
    "            timerange_hour = pd.date_range(timerange_month[idx].strftime('%Y-%m'),timerange_month[idx+1].strftime('%Y-%m'),freq='1h')\n",
    "\n",
    "            timerange_hour = timerange_hour[timerange_hour.month == timerange_month[idx].month]\n",
    "\n",
    "            timerange_hour_datev = []\n",
    "\n",
    "            for date_h in timerange_hour:\n",
    "                timerange_hour_datev.append(rpn_chris.date_to_datev(date_h))\n",
    "\n",
    "            try:\n",
    "                rmn.fstcloseall(fid)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # fid = rmn.fstopenall(path_sim_11km[idx],rmn.FST_RO)\n",
    "            fid = rmn.fstopenall(path_sim_2p5km[idx],rmn.FST_RO)\n",
    "            for j,phase in enumerate(var_list_simulation):\n",
    "\n",
    "                # print(f'getting {phase}')\n",
    "                for hidx,datev in enumerate(timerange_hour_datev):\n",
    "                    pbar.set_description(f'Processing {phase}: {timerange_month[idx].year}/{timerange_month[idx].month}/{timerange_hour[hidx].day} {timerange_hour[hidx].hour}:00 ')\n",
    "\n",
    "                    # key1 = rmn.fstinf(fid, nomvar=phase)\n",
    "                    if phase in ['TT']:\n",
    "                        ip1_1_5m = rmn.ip1_val(1.5, rmn.LEVEL_KIND_MGL)\n",
    "                        rec = rmn.fstlir(fid, nomvar=phase, datev=datev,ip1=ip1_1_5m)\n",
    "                    elif phase in ['UU','VV']:\n",
    "                        ip1_10m = rmn.ip1_val(10, rmn.LEVEL_KIND_MGL)\n",
    "                        rec = rmn.fstlir(fid, nomvar=phase, datev=datev,ip1=ip1_10m)\n",
    "                    else:\n",
    "                        rec = rmn.fstlir(fid, nomvar=phase, datev=datev)\n",
    "                    # rec = rmn.fstlir(fid, nomvar=phase, datev=datev)\n",
    "\n",
    "                    for station, subdf in dataframe_1h.groupby('filename'):\n",
    "                        subdf = subdf.loc[begin:end]\n",
    "\n",
    "                        if len(subdf.index)!=0:\n",
    "                            if subdf['#nan_1h'].sum()/(4*len(subdf.index))*100 < 10 or station is not ['GAREMANG','PIPMUA_G']:\n",
    "\n",
    "                                lon_stat = df_disdro.loc[station].X\n",
    "                                lat_stat = df_disdro.loc[station].Y\n",
    "\n",
    "                                if hidx ==0:\n",
    "                                    dict_gen[phase].append(0)\n",
    "                                    if j == 0:\n",
    "                                        dict_gen['time'].append(timerange_hour[hidx])\n",
    "                                        dict_gen['filename'].append(station)\n",
    "                                else:\n",
    "                                    mygrid = rmn.readGrid(fid,rec)              # Get the grid information for the (LAM) Grid -- Reads the tictac's\n",
    "                                    latlondict = rmn.gdll(mygrid)\n",
    "\n",
    "                                    lat_var = latlondict['lat']                     # Assign 'lat' to 2-D latitude field\n",
    "                                    lon_var = latlondict['lon']\n",
    "\n",
    "\n",
    "                                    if len(dict_idx_nearest_pt[station]) == 0:\n",
    "\n",
    "                                        pyt = (lon_var-(360+lon_stat))**2+(lat_var-lat_stat)**2\n",
    "                                        idx_lat,idx_lon = np.unravel_index(np.nanargmin(pyt), pyt.shape)\n",
    "                                        dict_idx_nearest_pt[station] = [idx_lat,idx_lon]\n",
    "\n",
    "                                        var_pt = [rec['d'][idx_lat,idx_lon]]\n",
    "                                    else:\n",
    "                                        idx_lat,idx_lon = dict_idx_nearest_pt[station]\n",
    "\n",
    "                                        var_pt = [rec['d'][idx_lat,idx_lon]]\n",
    "\n",
    "\n",
    "                                   # Close the RPN file\n",
    "                                   #  var_pt = rmn.gdllsval(latlondict, [lat_stat], [lon_stat], rec['d']) #Get value of var interpolated to lat/lon point\n",
    "\n",
    "                                    if phase == 'TT':\n",
    "                                        dict_gen[phase].append(var_pt[0])\n",
    "                                    elif phase in ['UU','VV']:\n",
    "                                        dict_gen[phase].append(var_pt[0]*0.514444)\n",
    "                                    elif phase in ['PR','RN','SN','FR','PE']:\n",
    "                                        dict_gen[phase].append(var_pt[0]* 1000* 3600)\n",
    "                                    else:\n",
    "                                        dict_gen[phase].append(var_pt[0])\n",
    "\n",
    "                                    if j == 0:\n",
    "                                        dict_gen['time'].append(timerange_hour[hidx])\n",
    "                                        dict_gen['filename'].append(station)\n",
    "\n",
    "                                pbar.update(1)\n",
    "\n",
    "            rmn.fstcloseall(fid)\n",
    "\n",
    "\n",
    "\n",
    "    df_sim_stat = pd.DataFrame.from_dict(dict_gen)\n",
    "\n",
    "    df_sim_stat.set_index('time',inplace=True)\n",
    "    df_sim_stat.sort_values(['filename','time'],inplace=True)\n",
    "\n",
    "    df_sim_stat.to_csv(f'/upslope/chalifour/projet_maitrise/data_sim_station/closest_point/dataset_stat_sim_2p5kmP3_{begin.strip(\"_\")}_{end.strip(\"_\")}_HR.csv')\n",
    "# df_sim_stat.to_csv(f'/upslope/chalifour/projet_maitrise/data_sim_station/closest_point_3d/dataset_stat_sim_11km_{begin.strip(\"_\")}_{end.strip(\"_\")}.csv')\n",
    "# df_sim_stat.to_csv(f'/upslope/chalifour/projet_maitrise/data_sim_station/dataset_stat_sim_11km_{begin.strip(\"_\")}_{end.strip(\"_\")}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T18:25:35.581963Z",
     "start_time": "2023-06-12T17:11:45.069286Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
